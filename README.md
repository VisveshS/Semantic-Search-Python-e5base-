In high-dim (**768**) semantic space, topology beats geometry
(curse of dimensionality), so analyzing KNN Relationship between embeddings is more meaningful

Embedding used : [huggingface e5-base-v2](https://huggingface.co/intfloat/e5-base-v2)

<img width="512" height="512" alt="sentenceembedding" src="https://github.com/user-attachments/assets/53911596-187f-494d-b249-01c87b024f3a" />
